
\documentclass[twocolumn]{book}

\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{times}
\usepackage{vmargin}

\usepackage[colorlinks=true,citecolor=blue,hyperfootnotes=false]
           {hyperref} % This must be the last package.

\newcommand{\doctitle}{How to Program a Computer}

% vmargin setup
\setpapersize{USletter}
\setmarginsrb%
{0.375in}%           left
{0.375in}%           top
{0.375in}%           right
{0.5in}%             bottom
{2\baselineskip}%    headheight
{2\baselineskip}%    headsep
{3\baselineskip}%    footheight
{4\baselineskip}%    footskip

% fancyhdr settings
\pagestyle{fancy}
\lhead{\sffamily\textbf{\doctitle}}
\chead{}
\rhead{\sffamily \thepage~of~\pageref{LastPage}}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
\lfoot{%
   \footnotesize\sffamily
   \begin{minipage}{0.95\textwidth}
   Copyright\ \copyright\ \ 2016\ \ Thomas E.\ Vaughan.
   Permission is granted to copy, distribute and/or modify this document under
   the terms of the GNU Free Documentation License, Version 1.3 or any later
   version published by the Free Software Foundation; with no Invariant
   Sections, no Front-Cover Texts, and no Back-Cover Texts.  A copy of the
   license is included in the section entitled ``GNU Free Documentation
   License''.
   \end{minipage}%
}
\cfoot{}
\rfoot{%
   \begin{minipage}{0.05\textwidth}
   \begin{flushright}
   \includegraphics[width=0.85\textwidth]{logo}
   \end{flushright}
   \end{minipage}%
}

\title{\doctitle}
\author{Thomas E. Vaughan}

\begin{document}

\frontmatter

\maketitle

\tableofcontents

\mainmatter

\chapter{Introduction}

In 1984, Apple Computer\footnote{%
   In 2007, the company changed its name from ``Apple Computer'' to ``Apple''.}
changed the world.  Introduced in that year, the Macintosh was a computer that
anybody could use.  Sales of the Macintosh began the widespread adoption of the
graphical user interface (GUI), which is conveniently controlled by a pointing
device (the mouse), as the standard interface for using a computer.  Even
without any knowledge of how to write a computer program, one could effectively
put the Macintosh to work.  Although easy use was possible with earlier
devices, the Macintosh was the first mass-market computing device that did not
even ship with programming tools.\footnote{%
   Programming tools were sold separately.}

Within ten years, basic knowledge of how to use a computer became common.
Today, each of the desktop workstation, the point-of-sale terminal, the tablet,
the cell phone, and even the occasional wrist watch is a device with more
computing power than the most advanced, room-sized computers of the 1960s.  The
computer is now an indispensable part of daily life because it can be
simultaneously small, computationally powerful, energy-efficient, inexpensive,
rugged, and easy to use. Widespread reliance on computers implies a
correspondingly widespread knowledge of their basic use.

However, knowledge of how to \emph{program} a computer---that is, knowledge of
\emph{advanced} usage---is not so widespread.  My use of ``basic'' and
``advanced'' here is not standard, but it serves to make clear the difference
between
\begin{itemize}
   \item what is basic, the usage of a computer without changing its behavior
      and
   \item what is advanced, the usage of a computer in order to change its
      behavior.
\end{itemize}

\section{Basic Usage}

A computer is the heart of many a device.  Let us define ``device'' in the
present context as ``a physical machine with both (a) behavior defined by
internal \emph{software} and (b) a \emph{sensory interface} for interacting
with a human user''.  Both good design of the interface and good software are
necessary for ease of use.  In the heart of the device, the software controls
the user's experience.

The sensory interface includes every
\begin{itemize}
   \item \emph{input} (e.g., keyboard) that the computer can sense and
   \item \emph{output} (e.g., monitor) that the user can sense.
\end{itemize}
Each input and output might itself be a separate physical device.  In any
event, the computer, the inputs, and the outputs comprise a single,
\emph{logical} device.

The software consists of the program or programs running in the computer.  By
experimenting with the sensory interface, the user can learn how the device
behaves.  When the user has learnt how to profit from the device's behavior,
the user has mastered \emph{basic} usage of the device's computer.  Basic usage
of a computer involves learning, but not changing, the behavioral rules that
are built in to the software.

\section{Advanced Usage}

If a device allow for \emph{advanced} usage, though, then the user can modify
in a deep way either the behavior of the device itself or the behavior of a
different device.\footnote{%
   In the advanced usage of a device, the user writes new software, which is
   deployed either on the same device or on a different device.  The device on
   which the new software is built is the \emph{host}.  The device for which
   the new software is built is called the \emph{target}.  When the target is a
   computer of a kind different from that of the host, the build is a
   \emph{cross} build.}
The advanced user writes new software.  The degree to which a device's behavior
can be changed is limited only by the device's physical construction, the
user's speculative intellect, and the user's available time to work.  Advanced
usage imposes certain requirements on the sensory interface and on the software
already in the device.  Most importantly, a device that allows for advanced
usage must provide an input interface for constructing a new computer program.

The more advanced the usage, the greater the degree to which the device's
behavior might be altered.  In its fullest sense, ``advanced usage'' in my
terminology refers to the construction of a program that can write to the
device's every sensory output, read from the device's every sensory input, and
be installed permanently in the device.  Advanced usage these days typically
involves
\begin{itemize}
   \item writing software in some combination of the C programming language and
      a scripting language and,
   \item by way of root or administrator privilege, installing the new software
      so that it runs with the appropriate level of permission to access the
      sensory interface.
\end{itemize}

\section{A Brief History of Computing}

Before 1949, there was no pre-existing software to aid in writing a new piece
of software.  New software was crafted, for example,
\begin{itemize}
   \item by manually flipping switches to configure individual bits in a
      computer's read-only memory (ROM),
   \item by punching holes in a paper tape and feeding the tape through a tape
      reader, or
   \item by punching holes in each of a set of cards and inserting a stack of
      cards into a card reader.
\end{itemize}
Each of the switch set, the tape reader, and the card reader is an input
interface.  Each corresponding input process would load the numbers of a
program's *machine code*, which governs the computer's behavior.

Beginning in 1949, with the EDSAC device\footnote{%
   The Electronic Delay Storage Automatic Calculator (EDSAC) was a tube-based
   electronic computer constructed at the University of Cambridge in England.
   The phrase "electronic delay" refers to mercury delay lines, which were used
   for dynamic memory.},
a special piece of software called an ``assembler'' could be installed.  An
assembler allows a program to be written in *assembly language* rather than
directly in machine-code numbers.  The assembler is itself a machine-code
program that translates assembly code into another machine-code program.  In
EDSAC, the machine code defining the assembler program was manually installed
by setting switches to configure ROM.  Then any other program could be written
by punching holes, corresponding to assembly instructions, into a paper tape.
A program was represented on a length of tape that was fed into a tape reader.
EDSAC's assembler translated the program on the tape into machine code stored
in dynamic memory, and then the new program could be run.  A problem with any
approach like this is that, usually, each new kind of computer has a new
assembly language.  So, in order for a program to run on the new computer, the
same program must be re-written by hand in the new assembly language.

Beginning in 1957 with IBM's FORTRAN language\footnote{%
   Each early version of the language was called ``FORTRAN'': FORTRAN (released
   in 1957), FORTRAN II (1958), FORTRAN IV (1962), FORTRAN 66 (1966), and
   FORTRAN 77 (1978).  Each more recent version of the language is called
   ``Fortran'': Fortran 90 (1991), Fortran 95 (1997), Fortran 2003 (2004), and
   Fortran 2008 (2010).  The name is based on the phrase, ``formula
   translation''.},
new software could be written as words in a \emph{portable programming
language}, usually called simply a ``programming language''.  In the 1950s and
1960s, each line of a computer program in FORTRAN was usually typed on a
mechanical keyboard that punched holes in a rigid card.  A special piece of
software already installed in the computer was the *compiler*.  Just as the
assembler translated assembly code into machine code, the compiler translated
the FORTRAN code into machine code (or, as an intermediate step, into assembly
code) so that the program could run directly on the computer as machine code.
The advantage of a programming language is portability: Even though a new
device might have a new set of machine instructions and a new assembly
language, the device could still be delivered with a compiler that takes as
input any program code written in the same, old programming language.  So the
same FORTRAN card stack could be fed into different devices.  Each device's
compiler would convert the FORTRAN code into the machine code appropriate to
the device.  By the early 1960s, electronic-keyboard terminals began to replace
punched cards at the device's input interface. By the early 1970s, a terminal
with keyboard and video monitor, rather than keyboard and paper, became common.
Still, punched cards were used for program input into the 1970s.

In 1975 MITS sold the first commercially successful kit (the Altair 8800) for a
small computing device to be used at home.  Before this, a computer was usually
a large device found at a government or business office.  The Altair 8800 still
had manual switches for loading a bare machine-code program into memory.

Since the late 1970s, new software has almost always been written, even for a
home device, with the assistance of existing software already running.
Starting in 1977, with Apple Computer's introduction of the Apple II, Radio
Shack's introduction of the TRS-80, and Commodore's introduction of the PET,
the software delivered in the home computer not only enabled the easy writing
of new software in a simple, high-level language like BASIC\footnote{%
   Beginner's All-purpose Symbolic Instruction Code (BASIC) refers to any of
   several similar programming languages designed to be easy to use.},
but also stimulated the rapid growth of a population of users who could profit
from merely a basic knowledge of how a computer operates.

By 1984, the consumer market was ready for Apple Computer's Macintosh.
Although the Macintosh was relatively easy to use in comparison with earlier
devices, it was also relatively difficult to program. A new Macintosh did not
ship with a programming language; also, the details of how the hardware and
installed software function were kept secret from the user.  The Macintosh was
just one example of a trend toward making the internal operation of a computer
inscrutable and toward making the writing of new, useful programs for a device
difficult or even impossible for the user.

In 1985, Richard Stallman, responding to the trend toward making the advanced
usage of a computer difficult or impossible, founded the non-profit Free
Software Foundation (FSF).  One of its purposes is to produce freely
distributable software that enables writing new software for any computer.  The
FSF's GNU project\footnote{%
   ``GNU'' is a recursive acronym that stands for ``GNU's Not Unixs''. Despite
   its name, the GNU project aims to provide a complete Unix-like operating
   system.  The Unix operating system was developed originally by AT\&T in the
   early 1970s. The C programming language was used to write most of Unix, and
   C similarly forms the basis for GNU.  So one of the most important tools
   provided by GNU is the C compiler.}
aims to produce a complete software system for running a computer.  The GNU
system is still incomplete, but many of its core programs\footnote{%
   Such as the GNU Compiler Collection (GCC).},
along with free software from other projects\footnote{%
   Such as the Linux kernel.},
do provide a complete, free system for writing new software.

In the early days of computing, every device allowed for advanced usage.  As
computing has evolved through history, ever more software has come to reside
permanently in the device.  Some of the resident software might enable the
writing of new programs or the modification of old programs, but, as the
evolution of computing has progressed, an ever smaller fraction of resident
software is typically installed to aid advanced usage.  Before 1980 or so, the
typical user of a computer was an advanced user; a common user's needs could be
met only if he wrote his own program.  By the middle of the 1980s, when Apple
Computer introduced the Macintosh, the basic, non-programming use of computing
devices was growing rapidly more common.  Since the early 1990s, when Microsoft
introduced a popular, standard GUI (Microsoft Windows 3.1) for
open-architecture personal computers, the typical computer user has had only
basic knowledge of how to use a computer.  Out of the box, a device purchased
today is already loaded with all of the software that the basic user needs.
Advanced usage can be difficult, expensive, or even impossible for some
devices.  The most convenient approach to enable advanced usage is often to
install software from the FSF.

\section{Purpose of the Book}

In what follows, the reader will learn about the advanced usage of a computer.
I begin by describing how a computer works.  Then I introduce some free tools
that, taken together, enable one to write new software in a Unix-like system.
After I introduce the suite of tools, I focus on a compiled language (C).  A
compiled language gives great control and speed while allowing a program to be
portable from one device to the next.  I also introduce a scripting language
(Ruby).  Like a compiled language, a scripting language allows the writing of
portable programs.  A scripting language, though, also allows a program to be
written most quickly and concisely.  Although a scripted program does not run
so fast as the corresponding compiled program, the speed of the script is often
fast enough.  Almost every programming project can be efficiently completed
through the development of some combination of compiled and scripted code.  By
referring only to free software (for example, the GNU C compiler and the
interpreter for Ruby scripts), I point to an easy way for the reader to acquire
the tools necessary for the advanced usage of a computer.  The material
herebelow gives the reader the power to define a device's behavior just as he
pleases, within the constraints of the device's physical capability.

\chapter{Advanced Usage of a Computer}

\section{How a Computer Works}

\subsection{Memory}

\subsubsection{Bit, Nybble, Byte, and Word}

\subsubsection{Address}

\subsection{Processor}

\subsubsection{Register}

\subsubsection{Operation}

\subsection{Simple Program}

\subsubsection{Machine Language}

\subsubsection{Assembly Language}

\subsection{Operating System}

\subsubsection{Kernel}

\subsubsection{Interrupt}

\subsubsection{Process}

\section{The Shell}

\subsection{Getting a Shell Prompt}

\subsection{Basic Use of the Shell}

\section{Finding Documentation}

\section{Editing a Text File}

\section{The Simplest C Program}

\chapter{Programming in the C Language}

\chapter{Programming in the Ruby Language}

\end{document}

